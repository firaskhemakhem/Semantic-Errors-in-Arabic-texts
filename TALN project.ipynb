{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea1c17b",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6a0d29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install utilities-package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "56bb05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c40d4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1a407ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "50c8412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================   \n",
    "# ====== N-Grams Models ======\n",
    "\n",
    "#t_model = gensim.models.Word2Vec.load('C:/Users/User/OneDrive/Bureau/full_grams_cbow_100_twitter/full_grams_cbow_100_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "286ed5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2d47a98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fasttext.util.download_model('ar', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7859033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bd21a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Charger les modèles de plongement lexical pré-entraînés\n",
    "unigram_model = gensim.models.KeyedVectors.load_word2vec_format(\"fast_text_model.vec\", binary=False)\n",
    "bigram_model = gensim.models.KeyedVectors.load_word2vec_format(\"model.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "47261962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "file_path = 'w2v_SG_300_5_400_10.model'\n",
    "word_embed_1 = gensim.models.Word2Vec.load(file_path) #load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "48c0febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'w2v_CBOW_300_5_400_10.model'\n",
    "word_embed_2 = gensim.models.Word2Vec.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7539666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a vector file\n",
    "word_embed_1.wv.save_word2vec_format('model_SG.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "788c2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a vector file\n",
    "word_embed_2.wv.save_word2vec_format('model_CBOW.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd33d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import res\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_fasttext = KeyedVectors.load_word2vec_format(\"fast_text_model.vec\")\n",
    "model_ = KeyedVectors.load_word2vec_format(\"model.vec\")\n",
    "model_CBOW = KeyedVectors.load_word2vec_format(\"model_CBOW.vec\")\n",
    "model_SG = KeyedVectors.load_word2vec_format(\"model_SG.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8998552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd238a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322ad8b",
   "metadata": {},
   "source": [
    "### One Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1fb38fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = model_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fa50456",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"يحإب علينا الذهاب باكرا \"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "acb04be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement \n",
    "\n",
    "input_sentence = re.sub(r'[^\\w\\s]','',input_sentence) # Supprimer les caractères spéciaux\n",
    "input_sentence = input_sentence.lower() # Convertir en minuscule\n",
    "\n",
    "# Initialisation des outils de traitement du langage naturel\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "# Initialisation des listes pour stocker les résultats\n",
    "corrected_sentence = []\n",
    "potential_errors = []\n",
    "correction = []\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "06b60efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يحإب n'est pas connus\n"
     ]
    }
   ],
   "source": [
    "vector_word = []\n",
    "for word in input_sentence.split():\n",
    "    if word in stop_words: # ignorer les mots vides\n",
    "        continue\n",
    "    try:\n",
    "        vector = ft_model.get_vector(word)  # création du vecteur de plangemnt lexical de chaque mot \n",
    "    except KeyError:\n",
    "        vector = np.zeros(ft_model.vector_size)  # si le mots ne fait pas partie de l'ensemble des mots connues par le modèle on remplit le vecteur avec des 0\n",
    "    vector_word.append(vector)\n",
    "    \n",
    "## afficher le mot erroné\n",
    "for i in range(len(vector_word)) :\n",
    "    if vector_word[i].all() == np.zeros(ft_model.vector_size).all() : \n",
    "        err_word = input_sentence.split()[i]\n",
    "        print( err_word, \"n'est pas connus\")\n",
    "        \n",
    "        \n",
    "## prédir la correction en se basant sur le vocabulaire du model et la distance minimal entre le lemme du mot et les autres mots du vocabulaire \n",
    "stemmed_word = stemmer.stem(err_word) # racine du mot erroné\n",
    "min_distance = float('inf')   # initialiser la valeur minimal de distance \n",
    "closest_word = None\n",
    "\n",
    "for vocab_word in ft_model.index_to_key:  ## parcourir le vocabulaire du modèle mis en évidence \n",
    "    stemmed_vocab_word = stemmer.stem(vocab_word)\n",
    "    distance = edit_distance(stemmed_word, stemmed_vocab_word)\n",
    "    if distance < min_distance:\n",
    "        min_distance = distance\n",
    "        closest_word = vocab_word\n",
    "        correction.append(closest_word)\n",
    "        d[min_distance] = vocab_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1021861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 يحب\n",
      "0 يحابي\n"
     ]
    }
   ],
   "source": [
    "## Afficher les deux mots les plus proches prédit comme correction\n",
    "for i in list(d.keys())[len(d.keys())-2:len(d.keys())]:\n",
    "    print(i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c975708",
   "metadata": {},
   "source": [
    "### Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e109d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_fasttext\n",
    "model2 = model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "61ba5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"يحإب علينا الذهاب باكرا \"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "03f6f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement \n",
    "\n",
    "input_sentence = re.sub(r'[^\\w\\s]','',input_sentence) # Supprimer les caractères spéciaux\n",
    "input_sentence = input_sentence.lower() # Convertir en minuscule\n",
    "\n",
    "# Initialisation des outils de traitement du langage naturel\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "# Initialisation des listes pour stocker les résultats\n",
    "corrected_sentence = []\n",
    "potential_errors = []\n",
    "correction = []\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3a7a712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_word = []\n",
    "## création du vécteur de plangement lexicale de chaque mot en faisant la concaténation entre les vecteurs obtenue de chaque modèle\n",
    "for word in input_sentence.split():\n",
    "    if word in stop_words: # ignorer les mots vides\n",
    "        continue\n",
    "    try:\n",
    "        vector1 = model1.get_vector(word)  # création du vecteur de plangemnt lexical de chaque mot \n",
    "    except KeyError:\n",
    "        vector1 = np.zeros(model1.vector_size)  # si le mots ne fait pas partie de l'ensemble des mots connues par le modèle on remplit le vecteur avec des 0\n",
    "    try:\n",
    "        vector2 = model2.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector2 = np.zeros(model2.vector_size)\n",
    "    combined_vector = np.concatenate([vector1, vector2]) ## la dimention = dim(v1) + dim(v2) (=200 dans notre cas)\n",
    "    vector_word.append(combined_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d229b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يحإب n'est pas connus\n"
     ]
    }
   ],
   "source": [
    "## afficher le mot erroné\n",
    "for i in range(len(vector_word)) :\n",
    "    if vector_word[i].all() == np.zeros(len(vector_word[0])).all() : \n",
    "        err_word = input_sentence.split()[i]\n",
    "        print( err_word, \"n'est pas connus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ccd36cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prédir la correction en se basant sur le vocabulaire du model et la distance minimal entre le lemme du mot et les autres mots du vocabulaire \n",
    "stemmed_word = stemmer.stem(err_word) # racine du mot erroné\n",
    "min_distance = float('inf')   # initialiser la valeur minimal de distance \n",
    "closest_word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c5e003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model1,model2]:\n",
    "    for vocab_word in model.index_to_key:  ## parcourir le vocabulaire du modèle mis en évidence \n",
    "        stemmed_vocab_word = stemmer.stem(vocab_word)\n",
    "        distance = edit_distance(stemmed_word, stemmed_vocab_word)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = vocab_word\n",
    "            correction.append(closest_word)\n",
    "            d[min_distance] = vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ad18cf2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 يحب\n",
      "0 يحابي\n"
     ]
    }
   ],
   "source": [
    "## Afficher les deux mots les plus proches prédit comme correction\n",
    "for i in list(d.keys())[len(d.keys())-2:len(d.keys())]:\n",
    "    print(i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc3fe2",
   "metadata": {},
   "source": [
    "### Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ec78770",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_fasttext\n",
    "model2 = model_\n",
    "model3 = model_CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fad6d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"يحإب علينا الذهاب باكرا \"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f34e91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement \n",
    "\n",
    "input_sentence = re.sub(r'[^\\w\\s]','',input_sentence) # Supprimer les caractères spéciaux\n",
    "input_sentence = input_sentence.lower() # Convertir en minuscule\n",
    "\n",
    "# Initialisation des outils de traitement du langage naturel\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "# Initialisation des listes pour stocker les résultats\n",
    "corrected_sentence = []\n",
    "potential_errors = []\n",
    "correction = []\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9788da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_word = []\n",
    "## création du vécteur de plangement lexicale de chaque mot en faisant la concaténation entre les vecteurs obtenue de chaque modèle\n",
    "for word in input_sentence.split():\n",
    "    if word in stop_words: # ignorer les mots vides\n",
    "        continue\n",
    "    try:\n",
    "        vector1 = model1.get_vector(word)  # création du vecteur de plangemnt lexical de chaque mot \n",
    "    except KeyError:\n",
    "        vector1 = np.zeros(model1.vector_size)  # si le mots ne fait pas partie de l'ensemble des mots connues par le modèle on remplit le vecteur avec des 0\n",
    "    try:\n",
    "        vector2 = model2.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector2 = np.zeros(model2.vector_size)\n",
    "    try:\n",
    "        vector3 = model3.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector3 = np.zeros(model3.vector_size)\n",
    "    combined_vector = np.concatenate([vector1, vector2,vector3]) ## la dimention = dim(v1) + dim(v2)+dim(v3)\n",
    "    vector_word.append(combined_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "30b49b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يحإب n'est pas connus\n"
     ]
    }
   ],
   "source": [
    "## afficher le mot erroné\n",
    "for i in range(len(vector_word)) :\n",
    "    if vector_word[i].all() == np.zeros(len(vector_word[0])).all() : \n",
    "        err_word = input_sentence.split()[i]\n",
    "        print( err_word, \"n'est pas connus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "17edbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prédir la correction en se basant sur le vocabulaire du model et la distance minimal entre le lemme du mot et les autres mots du vocabulaire \n",
    "stemmed_word = stemmer.stem(err_word) # racine du mot erroné\n",
    "min_distance = float('inf')   # initialiser la valeur minimal de distance \n",
    "closest_word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b9b7bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model1,model2,model3]:\n",
    "    for vocab_word in model.index_to_key:  ## parcourir le vocabulaire du modèle mis en évidence \n",
    "        stemmed_vocab_word = stemmer.stem(vocab_word)\n",
    "        distance = edit_distance(stemmed_word, stemmed_vocab_word)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = vocab_word\n",
    "            correction.append(closest_word)\n",
    "            d[min_distance] = vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a8ab66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 يحب\n",
      "0 يحابي\n"
     ]
    }
   ],
   "source": [
    "## Afficher les deux mots les plus proches prédit comme correction\n",
    "for i in list(d.keys())[len(d.keys())-2:len(d.keys())]:\n",
    "    print(i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886e426",
   "metadata": {},
   "source": [
    "### Four models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "286e8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_fasttext\n",
    "model2 = model_\n",
    "model3 = model_CBOW\n",
    "model4 = model_SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8b652094",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"يحإب علينا الذهاب باكرا \"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "575df918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement \n",
    "\n",
    "input_sentence = re.sub(r'[^\\w\\s]','',input_sentence) # Supprimer les caractères spéciaux\n",
    "input_sentence = input_sentence.lower() # Convertir en minuscule\n",
    "\n",
    "# Initialisation des outils de traitement du langage naturel\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "# Initialisation des listes pour stocker les résultats\n",
    "corrected_sentence = []\n",
    "potential_errors = []\n",
    "correction = []\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "47fbfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_word = []\n",
    "## création du vécteur de plangement lexicale de chaque mot en faisant la concaténation entre les vecteurs obtenue de chaque modèle\n",
    "for word in input_sentence.split():\n",
    "    if word in stop_words: # ignorer les mots vides\n",
    "        continue\n",
    "    try:\n",
    "        vector1 = model1.get_vector(word)  # création du vecteur de plangemnt lexical de chaque mot \n",
    "    except KeyError:\n",
    "        vector1 = np.zeros(model1.vector_size)  # si le mots ne fait pas partie de l'ensemble des mots connues par le modèle on remplit le vecteur avec des 0\n",
    "    try:\n",
    "        vector2 = model2.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector2 = np.zeros(model2.vector_size)\n",
    "    try:\n",
    "        vector3 = model3.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector3 = np.zeros(model3.vector_size)\n",
    "    try:\n",
    "        vector4 = model4.get_vector(word)\n",
    "    except KeyError:\n",
    "        vector4 = np.zeros(model4.vector_size)\n",
    "    combined_vector = np.concatenate([vector1, vector2,vector3,vector4]) ## la dimention = dim(v1) + dim(v2) (=200 dans notre cas)\n",
    "    vector_word.append(combined_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "883e1d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يحإب n'est pas connus\n"
     ]
    }
   ],
   "source": [
    "## afficher le mot erroné\n",
    "for i in range(len(vector_word)) :\n",
    "    if vector_word[i].all() == np.zeros(len(vector_word[0])).all() : \n",
    "        err_word = input_sentence.split()[i]\n",
    "        print( err_word, \"n'est pas connus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7c379db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prédir la correction en se basant sur le vocabulaire du model et la distance minimal entre le lemme du mot et les autres mots du vocabulaire \n",
    "stemmed_word = stemmer.stem(err_word) # racine du mot erroné\n",
    "min_distance = float('inf')   # initialiser la valeur minimal de distance \n",
    "closest_word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "226bdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model1,model2,model3,model4]:\n",
    "    for vocab_word in model.index_to_key:  ## parcourir le vocabulaire du modèle mis en évidence \n",
    "        stemmed_vocab_word = stemmer.stem(vocab_word)\n",
    "        distance = edit_distance(stemmed_word, stemmed_vocab_word)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = vocab_word\n",
    "            correction.append(closest_word)\n",
    "            d[min_distance] = vocab_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d6a87c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 يحب\n",
      "0 يحابي\n"
     ]
    }
   ],
   "source": [
    "## Afficher les deux mots les plus proches prédit comme correction\n",
    "for i in list(d.keys())[len(d.keys())-2:len(d.keys())]:\n",
    "    print(i,d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37d9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
